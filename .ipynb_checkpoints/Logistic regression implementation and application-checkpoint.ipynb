{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr = 0.01, num_iter = 10000, fit_intercept = True, verbose = False):\n",
    "        self.lr = lr\n",
    "        self.num_inter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis = 1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        return(-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if(self.fit_intercept):\n",
    "            X = self.__add_intercept(X)\n",
    "            \n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_inter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            if(self.verbose == True and i % 10000 == 0):\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print(f'loss: {self.__loss(h, y)} \\t')\n",
    "                \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "            \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, K, lr=0.01, num_iter=10000):\n",
    "        self.lr = lr\n",
    "        self.num_inter = num_iter\n",
    "        self.K = K\n",
    "\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "    def __softmax(self, z):\n",
    "        z -= np.max(z)\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1)\n",
    "\n",
    "    def __h(self, X, y):\n",
    "        return self.__softmax(X @ self.theta)\n",
    "\n",
    "    def __J(self, preds, y, m):\n",
    "        return np.sum(- np.log(preds[np.arange(m), y]))\n",
    "\n",
    "    def __T(self, y, K):\n",
    "        # one hot encoding\n",
    "        one_hot = np.zeros((len(y), K))\n",
    "        one_hot[np.arange(len(y)), y] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def __compute_gradient(self, theta, X, y, m):\n",
    "        preds = self.__h(X, theta)\n",
    "        gradient = 1 / m * X.T @ (preds - self.__T(y, self.K))\n",
    "        return gradient\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        hist = {'loss': [], 'acc': []}\n",
    "        m, n = X.shape\n",
    "\n",
    "        for i in range(self.__num.iter):\n",
    "            gradient = self.__compute_gradient(self.theta, X, y, m)\n",
    "            self.theta -= self.lr * gradient\n",
    "\n",
    "            # loss\n",
    "            preds = self.__h(X, self.theta)\n",
    "            loss = self.__J(preds, y, m)\n",
    "\n",
    "            c = 0\n",
    "            for j in range(len(y)):\n",
    "                if np.argmax(self.__h(X[j], self.theta)) == y[j]:\n",
    "                    c += 1\n",
    "                acc = c / len(y)\n",
    "                hist['acc'].append(acc)\n",
    "            # print stats\n",
    "            if i % 200 == 0: print('{:.2f} {:.2f}%', format(loss, acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SoftmaxRegression at 0x1c3fe3c4340>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SoftmaxRegression(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegression at 0x1c3fe3c42e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
